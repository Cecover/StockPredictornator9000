{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'Get rich quick' scheme, a 5-day prediction of FTSE/TWSE TW50 Index stock prices.\n",
    "\n",
    "Caveats:\n",
    "\n",
    "1. We used historical data from the beginning of 2020 (2020/01/02) to the 15th of March 2024\n",
    "2. We will be predicting the price index\n",
    "3. We write everything in Python\n",
    "4. We compare several models and will only submit one result which we think is the best\n",
    "\n",
    "Hard requirements:\n",
    "\n",
    "1. The dataset, which is a FTSE/TWSE TW-50 index stock from 2020-01-02 to 2024-03-15\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72eb0333b3681620",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ===== Imports =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T07:00:00.984619Z",
     "start_time": "2024-03-17T07:00:00.651181Z"
    }
   },
   "id": "2261133186b257d3",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ===== Data loading and plotting =====\n",
    "\n",
    "dataset = pd.read_csv('dataset/FTSE TWSE Taiwan 50 Index.csv')\n",
    "\n",
    "# Since we will be predicting the price index, we will be making a dataframe with the values we'd like to predict\n",
    "# I am lazy, so we just drop the unused columns instead\n",
    "df = dataset.drop(columns=['Total Return Index', 'Change', '%Change'])\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Now, we just need to plot the dataset\n",
    "fig = plt.subplots(figsize=(16, 5))\n",
    "plt.plot(df['Date'], df['Price Index'])\n",
    "plt.title('TW-50 Price Index', fontsize=20)\n",
    "plt.xlabel('Date', fontsize=15)\n",
    "plt.ylabel('Price', fontsize=15)\n",
    "plt.xticks(rotation=30, fontsize=15)\n",
    "plt.xlim(pd.Timestamp('2020-01-02'), pd.Timestamp('2024-03-15'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fbfb4ba6b6faffb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ===== Predicting stock prices using Transformers =====\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Complete transformer model for stock price prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 attention_dim: int, \n",
    "                 attention_dropout: float,\n",
    "                 attention_heads: int,\n",
    "                 hidden_dim: int,\n",
    "                 perceptron_dim: int,\n",
    "                 perceptron_dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_heads = attention_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.perceptron_dim = perceptron_dim\n",
    "        self.perceptron_dropout = perceptron_dropout\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(num_heads=self.attention_heads, embed_dim=self.attention_dim, dropout=self.attention_dropout)\n",
    "        self.norm1 = nn.LayerNorm(self.hidden_dim, elementwise_affine=False, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(self.hidden_dim, elementwise_affine=False, eps=1e-6)\n",
    "        \n",
    "        self.perceptron = nn.Sequential(\n",
    "            nn.Linear(self.perceptron_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.perceptron_dropout),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.perceptron_dim),\n",
    "            nn.Dropout(self.perceptron_dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        query, key, value = self.norm1(x)\n",
    "        \n",
    "        x = x + self.attention(query, key, value)[0]\n",
    "        x = x + self.perceptron(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "# # ===== Attention tester =====\n",
    "# attention_embedding_size = 128\n",
    "# attention_heads = 2\n",
    "# attention_dropout = 0.1\n",
    "# attention_hidden_dim = 128\n",
    "# perceptron_dim = 128\n",
    "# perceptron_dropout = 0.1\n",
    "# \n",
    "# x = torch.rand((3, 784, 128))\n",
    "# \n",
    "# attention = Transformer(attention_embedding_size, attention_dropout, attention_heads, attention_hidden_dim, perceptron_dim, perceptron_dropout)\n",
    "# context = attention(x)\n",
    "# print(\"Context final shape: \", context.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T09:30:10.799289Z",
     "start_time": "2024-03-17T09:30:10.795568Z"
    }
   },
   "id": "22f64c30a285ebef",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(**model_kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def calculate_loss(self, batch, mode='train'):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "355f9b13ea17799b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
